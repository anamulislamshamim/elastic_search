{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2578e34c",
   "metadata": {},
   "source": [
    "**Connect to Elasticsearch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7255aa62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Elasticsearch!\n",
      "{'cluster_name': 'docker-cluster',\n",
      " 'cluster_uuid': 'T1HeaWnRTOqX_BBgREVVbA',\n",
      " 'name': '64c49e436740',\n",
      " 'tagline': 'You Know, for Search',\n",
      " 'version': {'build_date': '2025-10-21T10:06:21.288851013Z',\n",
      "             'build_flavor': 'default',\n",
      "             'build_hash': '25d88452371273dd27356c98598287b669a03eae',\n",
      "             'build_snapshot': False,\n",
      "             'build_type': 'docker',\n",
      "             'lucene_version': '10.3.1',\n",
      "             'minimum_index_compatibility_version': '8.0.0',\n",
      "             'minimum_wire_compatibility_version': '8.19.0',\n",
      "             'number': '9.2.0'}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint \n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch(\"http://localhost:9200\")\n",
    "\n",
    "client_info = es.info()\n",
    "\n",
    "print(\"Connected to Elasticsearch!\")\n",
    "\n",
    "pprint(client_info.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f00cba",
   "metadata": {},
   "source": [
    "**1. Character filters**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e07172",
   "metadata": {},
   "source": [
    "1.1. HTML Strip Character Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "80e11109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': [{'end_offset': 26,\n",
      "             'position': 0,\n",
      "             'start_offset': 0,\n",
      "             'token': \"I'm so happy!\\n\",\n",
      "             'type': 'word'}]}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint \n",
    "\n",
    "response = es.indices.analyze(\n",
    "    char_filter=[\n",
    "        \"html_strip\"\n",
    "    ],\n",
    "    text=\"I&apos;m so happy</b>!</p>\",\n",
    ")\n",
    "\n",
    "pprint(response.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914da2eb",
   "metadata": {},
   "source": [
    "1.2. Mapping character filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7d9ab459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': [{'end_offset': 37,\n",
      "             'position': 0,\n",
      "             'start_offset': 0,\n",
      "             'token': 'I saw comet Tsuchinshan Atlas in 2024',\n",
      "             'type': 'word'}]}\n"
     ]
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    tokenizer=\"keyword\", \n",
    "    char_filter=[\n",
    "        {\n",
    "            \"type\": \"mapping\",\n",
    "            \"mappings\": [\n",
    "                \"٠ => 0\",\n",
    "                \"١ => 1\",\n",
    "                \"٢ => 2\",\n",
    "                \"٣ => 3\",\n",
    "                \"٤ => 4\",\n",
    "                \"٥ => 5\",\n",
    "                \"٦ => 6\",\n",
    "                \"٧ => 7\",\n",
    "                \"٨ => 8\",\n",
    "                \"٩ => 9\"\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    text=\"I saw comet Tsuchinshan Atlas in ٢٠٢٤\",\n",
    ")\n",
    "\n",
    "pprint(response.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c8dc07",
   "metadata": {},
   "source": [
    "**Tokenizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853b8512",
   "metadata": {},
   "source": [
    "2.1. Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2faa7719",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = es.indices.analyze(\n",
    "    tokenizer=\"standard\", \n",
    "    text=\"The 2 QUICK Brown-Foxes jumped over the lazy dog's bone.\",\n",
    ")\n",
    "\n",
    "tokens = response.body[\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f61a0be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: The, Type: <ALPHANUM>\n",
      "Token: 2, Type: <NUM>\n",
      "Token: QUICK, Type: <ALPHANUM>\n",
      "Token: Brown, Type: <ALPHANUM>\n",
      "Token: Foxes, Type: <ALPHANUM>\n",
      "Token: jumped, Type: <ALPHANUM>\n",
      "Token: over, Type: <ALPHANUM>\n",
      "Token: the, Type: <ALPHANUM>\n",
      "Token: lazy, Type: <ALPHANUM>\n",
      "Token: dog's, Type: <ALPHANUM>\n",
      "Token: bone, Type: <ALPHANUM>\n"
     ]
    }
   ],
   "source": [
    "for token in tokens:\n",
    "    print(f\"Token: {token['token']}, Type: {token[\"type\"]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4af2c5",
   "metadata": {},
   "source": [
    "2.2. Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7a4af1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: the, Type: word\n",
      "Token: quick, Type: word\n",
      "Token: brown, Type: word\n",
      "Token: foxes, Type: word\n",
      "Token: jumped, Type: word\n",
      "Token: over, Type: word\n",
      "Token: the, Type: word\n",
      "Token: lazy, Type: word\n",
      "Token: dog, Type: word\n",
      "Token: s, Type: word\n",
      "Token: bone, Type: word\n"
     ]
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    tokenizer=\"lowercase\",\n",
    "    text = \"The 2 QUICK Brown-Foxes jumped over the lazy dog's bone.\"\n",
    ")\n",
    "\n",
    "tokens = response.body['tokens']\n",
    "\n",
    "for token in tokens:\n",
    "    print(f\"Token: {token[\"token\"]}, Type: {token['type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87beaf5",
   "metadata": {},
   "source": [
    "**3. Token filter**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996932f2",
   "metadata": {},
   "source": [
    "3.1. Apostrophe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dbb5ccd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: The, Type: <ALPHANUM>\n",
      "Token: 2, Type: <NUM>\n",
      "Token: QUICK, Type: <ALPHANUM>\n",
      "Token: Brown, Type: <ALPHANUM>\n",
      "Token: Foxes, Type: <ALPHANUM>\n",
      "Token: jumped, Type: <ALPHANUM>\n",
      "Token: over, Type: <ALPHANUM>\n",
      "Token: the, Type: <ALPHANUM>\n",
      "Token: lazy, Type: <ALPHANUM>\n",
      "Token: dog, Type: <ALPHANUM>\n",
      "Token: bone, Type: <ALPHANUM>\n"
     ]
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    tokenizer=\"standard\", \n",
    "    text=\"The 2 QUICK Brown-Foxes jumped over the lazy dog's bone.\",\n",
    "    filter=[\n",
    "        \"apostrophe\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "tokens = response.body['tokens']\n",
    "for token in tokens:\n",
    "    print(f\"Token: {token['token']}, Type: {token[\"type\"]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ded18b",
   "metadata": {},
   "source": [
    "3.2. Decimal digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "939f2d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: I\n",
      "Token: saw\n",
      "Token: comet\n",
      "Token: Tsuchinshan\n",
      "Token: Atlas\n",
      "Token: in\n",
      "Token: 2024\n"
     ]
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    tokenizer=\"standard\", \n",
    "    text=\"I saw comet Tsuchinshan Atlas in ٢٠٢٤\",\n",
    "    filter=[\n",
    "        \"decimal_digit\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "tokens = response.body['tokens']\n",
    "\n",
    "for token in tokens:\n",
    "    print(f\"Token: {token['token']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f42a04",
   "metadata": {},
   "source": [
    "3.3. Reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "baa90dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: I\n",
      "Token: was\n",
      "Token: temoc\n",
      "Token: nahsnihcusT\n",
      "Token: saltA\n",
      "Token: ni\n",
      "Token: ٤٢٠٢\n"
     ]
    }
   ],
   "source": [
    "result = es.indices.analyze(\n",
    "    tokenizer=\"standard\", \n",
    "    text=\"I saw comet Tsuchinshan Atlas in ٢٠٢٤\", \n",
    "    filter=[\n",
    "        \"reverse\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "tokens = result.body['tokens']\n",
    "\n",
    "for token in tokens:\n",
    "    print(f\"Token: {token['token']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2815dd",
   "metadata": {},
   "source": [
    "**4. Built-in analyzers**\n",
    "# \n",
    "4.1. Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a1045d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 'i'\n",
      "Token: 'saw'\n",
      "Token: 'comet'\n",
      "Token: 'tsuchinshan'\n",
      "Token: 'atlas'\n",
      "Token: 'in'\n",
      "Token: '٢٠٢٤'\n"
     ]
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    analyzer=\"standard\",\n",
    "    text=\"I saw comet Tsuchinshan Atlas in ٢٠٢٤\",\n",
    ")\n",
    "tokens = response.body[\"tokens\"]\n",
    "for token in tokens:\n",
    "    print(f\"Token: '{token['token']}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a46e24",
   "metadata": {},
   "source": [
    "4.2. Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "53062a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 'i'\n",
      "Token: 'saw'\n",
      "Token: 'comet'\n",
      "Token: 'tsuchinshan'\n",
      "Token: 'atlas'\n"
     ]
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    analyzer=\"stop\",\n",
    "    text=\"I saw comet. Tsuchinshan Atlas in ٢٠٢٤\",\n",
    ")\n",
    "tokens = response.body[\"tokens\"]\n",
    "for token in tokens:\n",
    "    print(f\"Token: '{token['token']}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c35ff4f",
   "metadata": {},
   "source": [
    "4.3. Keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "85dbe5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 'I saw comet Tsuchinshan Atlas in ٢٠٢٤'\n"
     ]
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    analyzer=\"keyword\",\n",
    "    text=\"I saw comet Tsuchinshan Atlas in ٢٠٢٤\",\n",
    ")\n",
    "tokens = response.body[\"tokens\"]\n",
    "for token in tokens:\n",
    "    print(f\"Token: '{token['token']}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b4f6a1",
   "metadata": {},
   "source": [
    "**5. Index time VS Search time analysis**\n",
    "#\n",
    "5.1. Index time\n",
    "#\n",
    "Index-time analysis transforms text before it's stored in the index. In this example, let's create an index with an analyzer that lowercases text, removes HTML tags, and replaces ampersands (&) with the word \"and.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "86289bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acknowledged': True,\n",
      " 'index': 'index_time_example',\n",
      " 'shards_acknowledged': True}\n"
     ]
    }
   ],
   "source": [
    "index_name = \"index_time_example\"\n",
    "settings = {\n",
    "    \"settings\": {\n",
    "        \"analysis\": {\n",
    "            \"char_filter\": {\n",
    "                \"ampersand_replacement\": {\n",
    "                    \"type\": \"mapping\",\n",
    "                    \"mappings\": [\"& => and\"]\n",
    "                }\n",
    "            },\n",
    "            \"analyzer\": {\n",
    "                \"custom_index_analyzer\": {\n",
    "                    \"type\": \"custom\",\n",
    "                    \"char_filter\": [\"html_strip\", \"ampersand_replacement\"],\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\"lowercase\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"content\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"custom_index_analyzer\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# delete and create index\n",
    "es.indices.delete(index=index_name, ignore_unavailable=True)\n",
    "response = es.indices.create(index=index_name, body=settings)\n",
    "\n",
    "pprint(response.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604d50a9",
   "metadata": {},
   "source": [
    "**Index Document**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7defb44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': 'x8BgZJoBlg0ijCqcq7PV',\n",
      " '_index': 'index_time_example',\n",
      " '_primary_term': 1,\n",
      " '_seq_no': 0,\n",
      " '_shards': {'failed': 0, 'successful': 1, 'total': 2},\n",
      " '_version': 1,\n",
      " 'result': 'created'}\n"
     ]
    }
   ],
   "source": [
    "document = {\n",
    "    \"content\": \"Visit my website https://myuniversehub.com/ & like some images!\"\n",
    "}\n",
    "\n",
    "response = es.index(\n",
    "    index=index_name,\n",
    "    body=document\n",
    ")\n",
    "\n",
    "pprint(response.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2702e5",
   "metadata": {},
   "source": [
    "When searching for the document, you'll notice that the content appears unchanged. This is expected because Elasticsearch stores the transformed tokens in an inverted index for searching purposes, while keeping the original document intact in the _source field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ec6f9783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"{'content': 'Visit my website https://myuniversehub.com/ & like some images!'}\"\n"
     ]
    }
   ],
   "source": [
    "result = es.search(\n",
    "    index=index_name,\n",
    "    body={\n",
    "        \"query\":  {\n",
    "            \"match_all\": {}\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "hits = result.body[\"hits\"][\"hits\"]\n",
    "\n",
    "for hit in hits:\n",
    "    pprint(f\"{hit[\"_source\"]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40c01d7",
   "metadata": {},
   "source": [
    "We can verify that the custom analyzer is working by applying it to the document like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "63f40cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: visit\n",
      "Token: my\n",
      "Token: website\n",
      "Token: https\n",
      "Token: myuniversehub.com\n",
      "Token: and\n",
      "Token: like\n",
      "Token: some\n",
      "Token: images\n"
     ]
    }
   ],
   "source": [
    "response = es.indices.analyze(\n",
    "    index=index_name,\n",
    "    body={\n",
    "        \"field\": \"content\",\n",
    "        \"text\": \"Visit my website https://myuniversehub.com/ & like some images!\"\n",
    "    }\n",
    ")\n",
    "\n",
    "tokens = response.body[\"tokens\"]\n",
    "for token in tokens:\n",
    "    print(f\"Token: {token[\"token\"]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f48f85",
   "metadata": {},
   "source": [
    "**5.2. Search time**\n",
    "#\n",
    "Search-time analysis transforms text only when a search query is performed, not when data is indexed. In this example, we’ll perform a search with a search-time analyzer that transforms text differently (e.g., it lowercases and removes stop words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a5ebde2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': 'Visit my website https://myuniversehub.com/ & like some images!'}\n"
     ]
    }
   ],
   "source": [
    "result = es.search(\n",
    "    index=index_name,\n",
    "    body={\n",
    "        \"query\": {\n",
    "            \"match\": { # match it for full-text search\n",
    "                \"content\": {\n",
    "                    \"query\": \"myuniversehub.com\",\n",
    "                    \"analyzer\": \"standard\" # use different analyzer than you use during indexing\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "hits = result[\"hits\"][\"hits\"]\n",
    "\n",
    "for hit in hits:\n",
    "    print(f\"{hit[\"_source\"]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8084c5",
   "metadata": {},
   "source": [
    "You can also use a term query to match exact terms. Since myuniversehub.com exists exactly as-is in the document, this query will return the document in the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "53c08604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': 'Visit my website https://myuniversehub.com/ & like some images!'}\n"
     ]
    }
   ],
   "source": [
    "result = es.search(\n",
    "    index=index_name,\n",
    "    body={\n",
    "        \"query\": {\n",
    "            \"term\": { # term is used for exact match\n",
    "                \"content\": {\n",
    "                    \"value\": \"myuniversehub.com\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "hits = result[\"hits\"][\"hits\"]\n",
    "\n",
    "for hit in hits:\n",
    "    print(f\"{hit[\"_source\"]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6e4355",
   "metadata": {},
   "source": [
    "Term is case-sensitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "515fc60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = es.search(\n",
    "    index=index_name,\n",
    "    body={\n",
    "        \"query\": {\n",
    "            \"term\": { # term is used for exact match\n",
    "                \"content\": {\n",
    "                    \"value\": \"Myuniversehub.com\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "hits = result[\"hits\"][\"hits\"]\n",
    "\n",
    "for hit in hits:\n",
    "    print(f\"{hit[\"_source\"]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b114307c",
   "metadata": {},
   "source": [
    "No result return because there is no Myuniversehub.com in content."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "denv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
